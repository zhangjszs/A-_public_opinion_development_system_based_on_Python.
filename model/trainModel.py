"""
sentiment_model.py  â€”â€” 2025â€‘05â€‘04 å®Œæ•´ä¿®æ­£ç‰ˆ
ä¾èµ–: pandas numpy scikit-learn matplotlib seaborn joblib
å¯é€‰: imbalancedâ€‘learn (è‹¥éœ€ SMOTE ç­‰è¿‡é‡‡æ ·)
"""
from __future__ import annotations
import warnings
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from pathlib import Path
from collections import Counter

from sklearn.model_selection import (
    train_test_split,
    StratifiedKFold,
    cross_validate,
)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    confusion_matrix,
    ConfusionMatrixDisplay,
    classification_report,
    RocCurveDisplay,
    f1_score,
    balanced_accuracy_score,
)
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.class_weight import compute_class_weight

# -------------------------------------------------------------------
# Matplotlib ä¸­æ–‡æ”¯æŒ
# -------------------------------------------------------------------
import matplotlib
matplotlib.rcParams["font.family"] = ["SimHei", "Microsoft YaHei", "SimSun"]
matplotlib.rcParams["axes.unicode_minus"] = False


# -------------------------------------------------------------------
# æ•°æ®åŠ è½½
# -------------------------------------------------------------------
def load_data(csv_path: str | Path) -> pd.DataFrame:
    """è¯»å– csv ä¸º DataFrameï¼Œå¹¶åšåŸºæœ¬æ¸…æ´—"""
    return (
        pd.read_csv(csv_path, header=None, names=["text", "label"])
        .dropna(subset=["text", "label"])
        .drop_duplicates()
        .reset_index(drop=True)
    )


# -------------------------------------------------------------------
# æœ´ç´ è´å¶æ–¯åŠ æƒç‰ˆï¼ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡ï¼‰
# -------------------------------------------------------------------
class WeightedMultinomialNB(MultinomialNB):
    """åœ¨ fit æ—¶å¯¹å°‘æ•°ç±»è‡ªåŠ¨åŠ æƒ"""

    def fit(self, X, y):
        classes = np.unique(y)
        weights = compute_class_weight("balanced", classes=classes, y=y)
        sample_weight = np.array([dict(zip(classes, weights))[label] for label in y])
        return super().fit(X, y, sample_weight=sample_weight)


# -------------------------------------------------------------------
# æ¨¡å‹åˆ—è¡¨
# -------------------------------------------------------------------
MODELS = {
    "NaiveBayes": WeightedMultinomialNB(),
    "LogReg": LogisticRegression(max_iter=1000, class_weight="balanced"),
    "LinearSVM": LinearSVC(class_weight="balanced"),
    "RandomForest": RandomForestClassifier(
        n_estimators=300, n_jobs=-1, random_state=42, class_weight="balanced"
    ),
}


def build_pipeline(estimator):
    """ç»Ÿä¸€çš„ TFâ€‘IDF + åˆ†ç±»å™¨æµæ°´çº¿"""
    return Pipeline(
        steps=[
            ("tfidf", TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
            ("clf", estimator),
        ]
    )


# -------------------------------------------------------------------
# äº¤å‰éªŒè¯ + å¯è§†åŒ–
# -------------------------------------------------------------------
def evaluate_models(
    df: pd.DataFrame,
    k_folds: int | None = None,
    scoring: str = "macro_f1",
) -> dict[str, np.ndarray]:
    """
    äº¤å‰éªŒè¯æ¯”è¾ƒå„æ¨¡å‹ã€‚
    scoring å–å€¼ï¼š'macro_f1' | 'bal_acc' | 'accuracy'
    """
    X, y = df["text"], df["label"]

    # åŠ¨æ€å†³å®š n_splitsï¼Œé¿å…æœ€å°ç±»åˆ« < n_splits
    min_class = y.value_counts().min()
    n_splits = k_folds or max(2, min(5, min_class))
    if min_class < 10:
        warnings.warn(
            f"âš ï¸ æœ€å°ç±»åˆ«æ ·æœ¬æ•°åªæœ‰ {min_class}ï¼Œå·²è‡ªåŠ¨å°† n_splits è®¾ä¸º {n_splits}ã€‚",
            UserWarning,
        )

    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    # å¤šæŒ‡æ ‡
    cv_metrics = {
        "accuracy": "accuracy",
        "macro_f1": "f1_macro",
        "bal_acc": "balanced_accuracy",
    }

    results = {}

    for name, est in MODELS.items():
        pipe = build_pipeline(est)
        cv_res = cross_validate(
            pipe,
            X,
            y,
            cv=skf,
            scoring=cv_metrics,
            n_jobs=-1,
            return_train_score=False,
        )
        key = f"test_{scoring}"      # è‡ªåŠ¨åŠ å‰ç¼€
        results[name] = cv_res[key]
        print(
            f"{name:<12} | {scoring}={cv_res[key].mean():.4f}Â±{cv_res[key].std():.4f} "
            f"| acc={cv_res['test_accuracy'].mean():.4f}"
        )

    # ç®±çº¿å›¾
    plt.figure(figsize=(8, 5))
    sns.boxplot(data=pd.DataFrame(results))
    plt.title(f"{n_splits}-Fold {scoring} Comparison")
    plt.ylabel(scoring)
    plt.tight_layout()
    plt.show()

    return results


# -------------------------------------------------------------------
# è®­ç»ƒæœ€ç»ˆæ¨¡å‹ + æ··æ·†çŸ©é˜µ / ROC
# -------------------------------------------------------------------
def train_best_model(
    df: pd.DataFrame, model_name: str = "NaiveBayes"
):
    X_train, X_test, y_train, y_test = train_test_split(
        df["text"],
        df["label"],
        test_size=0.2,
        stratify=df["label"],
        random_state=42,
    )
    pipe = build_pipeline(MODELS[model_name])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    print("\nğŸ¯ Classification Report")
    print(classification_report(y_test, y_pred, digits=4))

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))
    ConfusionMatrixDisplay(cm, display_labels=np.unique(y_test)).plot(
        cmap="Blues", xticks_rotation=45
    )
    plt.title(f"{model_name} Confusion Matrix")
    plt.tight_layout()
    plt.show()

    # ROCï¼ˆä»…äºŒåˆ†ç±»ä¸”æ”¯æŒ decision_functionï¼‰
    if len(np.unique(y_test)) == 2 and hasattr(pipe, "decision_function"):
        RocCurveDisplay.from_predictions(y_test, pipe.decision_function(X_test))
        plt.title(f"{model_name} ROC Curve")
        plt.tight_layout()
        plt.show()

    # ä¿å­˜æ¨¡å‹
    joblib.dump(pipe, "best_sentiment_model.pkl")
    print("âœ… æ¨¡å‹å·²ä¿å­˜ä¸º best_sentiment_model.pkl")
    return pipe


# -------------------------------------------------------------------
# æ¨ç†æ¥å£ï¼ˆå•ä¾‹ï¼‰
# -------------------------------------------------------------------
class SentimentPredictor:
    _model = None

    @classmethod
    def load(cls, path: str = "best_sentiment_model.pkl"):
        if cls._model is None:
            cls._model = joblib.load(path)
        return cls._model

    @classmethod
    def predict(cls, text: str):
        return cls.load().predict([text])[0]


# -------------------------------------------------------------------
# ä¸»æµç¨‹
# -------------------------------------------------------------------
if __name__ == "__main__":
    df = load_data("target.csv")
    print("ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:", Counter(df["label"]))

    # â‘  äº¤å‰éªŒè¯æ¯”è¾ƒ
    evaluate_models(df, scoring="macro_f1")

    # â‘¡ è®­ç»ƒæœ€ä¼˜æ¨¡å‹ï¼ˆçœ‹ä¸Šä¸€æ­¥ç»“æœæ‰‹åŠ¨æ”¹åå³å¯ï¼‰
    trained_pipe = train_best_model(df, model_name="NaiveBayes")

    # â‘¢ æ¨ç†ç¤ºä¾‹
    demo = "ç³Ÿç³•é€äº†"
    print("â›… é¢„æµ‹ç»“æœ:", SentimentPredictor.predict(demo))
