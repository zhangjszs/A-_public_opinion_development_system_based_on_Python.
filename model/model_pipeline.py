#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„æ¨¡å‹å¤„ç†æµæ°´çº¿æ§åˆ¶å™¨
æ•´åˆæ‰€æœ‰ä¿®å¤åçš„æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£
"""

import os
import sys
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / 'utils'))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(project_root / 'logs' / 'model_pipeline.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ModelPipeline:
    """å®Œæ•´çš„æ¨¡å‹å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, model_dir: str = None):
        self.model_dir = Path(model_dir) if model_dir else Path(__file__).parent
        self.project_root = self.model_dir.parent
        
        # åˆå§‹åŒ–å­æ¨¡å—
        self.data_processor = None
        self.sentiment_analyzer = None
        self.frequency_analyzer = None
        
        # æµæ°´çº¿çŠ¶æ€
        self.pipeline_status = {
            'data_processing': False,
            'sentiment_analysis': False,
            'frequency_analysis': False,
            'start_time': None,
            'end_time': None
        }
        
        logger.info("æ¨¡å‹æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_modules(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰å¤„ç†æ¨¡å—"""
        try:
            # å¯¼å…¥ä¿®å¤åçš„æ¨¡å—
            from improved_index import ModelDataProcessor
            from improved_yuqing import SentimentAnalyzer
            from improved_ciPingTotal import WordFrequencyAnalyzer
            
            # åˆ›å»ºå®ä¾‹
            self.data_processor = ModelDataProcessor(str(self.model_dir))
            self.sentiment_analyzer = SentimentAnalyzer(str(self.model_dir))
            self.frequency_analyzer = WordFrequencyAnalyzer(str(self.model_dir))
            
            logger.info("æ‰€æœ‰æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except ImportError as e:
            logger.error(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            return False
        except Exception as e:
            logger.error(f"æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def run_data_processing(self) -> bool:
        """è¿è¡Œæ•°æ®å¤„ç†æ­¥éª¤"""
        logger.info("ğŸ“Š å¼€å§‹æ•°æ®å¤„ç†...")
        
        try:
            if not self.data_processor:
                logger.error("æ•°æ®å¤„ç†å™¨æœªåˆå§‹åŒ–")
                return False
            
            success = self.data_processor.process_data_pipeline()
            self.pipeline_status['data_processing'] = success
            
            if success:
                logger.info("âœ… æ•°æ®å¤„ç†å®Œæˆ")
            else:
                logger.error("âŒ æ•°æ®å¤„ç†å¤±è´¥")
            
            return success
            
        except Exception as e:
            logger.error(f"æ•°æ®å¤„ç†å¼‚å¸¸: {e}")
            return False
    
    def run_sentiment_analysis(self) -> bool:
        """è¿è¡Œæƒ…æ„Ÿåˆ†ææ­¥éª¤"""
        logger.info("ğŸ˜Š å¼€å§‹æƒ…æ„Ÿåˆ†æ...")
        
        try:
            if not self.sentiment_analyzer:
                logger.error("æƒ…æ„Ÿåˆ†æå™¨æœªåˆå§‹åŒ–")
                return False
            
            success = self.sentiment_analyzer.run_analysis_pipeline()
            self.pipeline_status['sentiment_analysis'] = success
            
            if success:
                logger.info("âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")
            else:
                logger.error("âŒ æƒ…æ„Ÿåˆ†æå¤±è´¥")
            
            return success
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¼‚å¸¸: {e}")
            return False
    
    def run_frequency_analysis(self) -> bool:
        """è¿è¡Œè¯é¢‘åˆ†ææ­¥éª¤"""
        logger.info("ğŸ“ˆ å¼€å§‹è¯é¢‘åˆ†æ...")
        
        try:
            if not self.frequency_analyzer:
                logger.error("è¯é¢‘åˆ†æå™¨æœªåˆå§‹åŒ–")
                return False
            
            success = self.frequency_analyzer.run_frequency_analysis()
            self.pipeline_status['frequency_analysis'] = success
            
            if success:
                logger.info("âœ… è¯é¢‘åˆ†æå®Œæˆ")
            else:
                logger.error("âŒ è¯é¢‘åˆ†æå¤±è´¥")
            
            return success
            
        except Exception as e:
            logger.error(f"è¯é¢‘åˆ†æå¼‚å¸¸: {e}")
            return False
    
    def generate_pipeline_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š"""
        try:
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = None
            if self.pipeline_status['start_time'] and self.pipeline_status['end_time']:
                execution_time = (
                    self.pipeline_status['end_time'] - 
                    self.pipeline_status['start_time']
                ).total_seconds()
            
            # ç»Ÿè®¡æˆåŠŸæ­¥éª¤
            successful_steps = sum(1 for status in self.pipeline_status.values() if status is True)
            total_steps = 3  # æ•°æ®å¤„ç†ã€æƒ…æ„Ÿåˆ†æã€è¯é¢‘åˆ†æ
            
            # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
            output_files = []
            for file_path in [
                self.model_dir / 'comment_1_fenci.txt',
                self.model_dir / 'target.csv',
                self.model_dir / 'comment_1_fenci_qutingyongci_cipin.csv',
                self.model_dir / 'analysis_summary.json',
                self.model_dir / 'frequency_report.json'
            ]:
                if file_path.exists():
                    output_files.append({
                        'file': str(file_path),
                        'size': file_path.stat().st_size,
                        'modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                    })
            
            report = {
                'pipeline_status': self.pipeline_status,
                'execution_time_seconds': execution_time,
                'success_rate': f"{successful_steps}/{total_steps}",
                'output_files': output_files,
                'timestamp': datetime.now().isoformat()
            }
            
            return report
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return {}
    
    def save_pipeline_report(self, report: Dict[str, Any]) -> bool:
        """ä¿å­˜æµæ°´çº¿æŠ¥å‘Š"""
        try:
            import json
            
            report_file = self.model_dir / f'pipeline_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æµæ°´çº¿æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    def run_complete_pipeline(self, skip_on_error: bool = False) -> bool:
        """è¿è¡Œå®Œæ•´çš„æ¨¡å‹å¤„ç†æµæ°´çº¿"""
        logger.info("ğŸš€ å¯åŠ¨å®Œæ•´æ¨¡å‹å¤„ç†æµæ°´çº¿...")
        
        try:
            # è®°å½•å¼€å§‹æ—¶é—´
            self.pipeline_status['start_time'] = datetime.now()
            
            # 1. åˆå§‹åŒ–æ¨¡å—
            if not self._initialize_modules():
                logger.error("æ¨¡å—åˆå§‹åŒ–å¤±è´¥ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return False
            
            # 2. æ•°æ®å¤„ç†
            data_success = self.run_data_processing()
            if not data_success and not skip_on_error:
                logger.error("æ•°æ®å¤„ç†å¤±è´¥ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return False
            
            # 3. æƒ…æ„Ÿåˆ†æ
            sentiment_success = self.run_sentiment_analysis()
            if not sentiment_success and not skip_on_error:
                logger.error("æƒ…æ„Ÿåˆ†æå¤±è´¥ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return False
            
            # 4. è¯é¢‘åˆ†æ
            frequency_success = self.run_frequency_analysis()
            if not frequency_success and not skip_on_error:
                logger.error("è¯é¢‘åˆ†æå¤±è´¥ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return False
            
            # è®°å½•ç»“æŸæ—¶é—´
            self.pipeline_status['end_time'] = datetime.now()
            
            # 5. ç”Ÿæˆå’Œä¿å­˜æŠ¥å‘Š
            report = self.generate_pipeline_report()
            if report:
                self.save_pipeline_report(report)
            
            # åˆ¤æ–­æ€»ä½“æˆåŠŸ
            overall_success = all([
                data_success,
                sentiment_success, 
                frequency_success
            ])
            
            if overall_success:
                logger.info("ğŸ‰ å®Œæ•´æ¨¡å‹æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ!")
            else:
                logger.warning("âš ï¸  æµæ°´çº¿éƒ¨åˆ†æ­¥éª¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            
            return overall_success
            
        except Exception as e:
            logger.error(f"æµæ°´çº¿æ‰§è¡Œå¼‚å¸¸: {e}")
            self.pipeline_status['end_time'] = datetime.now()
            return False
    
    def run_step_by_step(self) -> bool:
        """é€æ­¥è¿è¡Œæµæ°´çº¿ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰"""
        logger.info("ğŸ” å¯åŠ¨é€æ­¥æ‰§è¡Œæ¨¡å¼...")
        
        try:
            # åˆå§‹åŒ–
            if not self._initialize_modules():
                return False
            
            self.pipeline_status['start_time'] = datetime.now()
            
            # é€æ­¥æ‰§è¡Œ
            steps = [
                ("æ•°æ®å¤„ç†", self.run_data_processing),
                ("æƒ…æ„Ÿåˆ†æ", self.run_sentiment_analysis),
                ("è¯é¢‘åˆ†æ", self.run_frequency_analysis)
            ]
            
            for step_name, step_func in steps:
                logger.info(f"ğŸ‘‰ æ­£åœ¨æ‰§è¡Œ: {step_name}")
                
                try:
                    success = step_func()
                    if success:
                        logger.info(f"âœ… {step_name} å®Œæˆ")
                    else:
                        logger.error(f"âŒ {step_name} å¤±è´¥")
                        
                        # è¯¢é—®æ˜¯å¦ç»§ç»­
                        user_input = input(f"{step_name} å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­ä¸‹ä¸€æ­¥ï¼Ÿ(y/n): ").lower()
                        if user_input != 'y':
                            logger.info("ç”¨æˆ·é€‰æ‹©ç»ˆæ­¢æµæ°´çº¿")
                            return False
                        
                except Exception as e:
                    logger.error(f"{step_name} å¼‚å¸¸: {e}")
                    user_input = input(f"{step_name} å¼‚å¸¸ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").lower()
                    if user_input != 'y':
                        return False
            
            self.pipeline_status['end_time'] = datetime.now()
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_pipeline_report()
            if report:
                self.save_pipeline_report(report)
            
            logger.info("ğŸ¯ é€æ­¥æ‰§è¡Œæ¨¡å¼å®Œæˆ")
            return True
            
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
            return False
        except Exception as e:
            logger.error(f"é€æ­¥æ‰§è¡Œå¼‚å¸¸: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæµæ°´çº¿
        pipeline = ModelPipeline()
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        import argparse
        parser = argparse.ArgumentParser(description='æ¨¡å‹å¤„ç†æµæ°´çº¿')
        parser.add_argument('--mode', choices=['complete', 'step', 'data', 'sentiment', 'frequency'], 
                          default='complete', help='æ‰§è¡Œæ¨¡å¼')
        parser.add_argument('--skip-on-error', action='store_true', 
                          help='é‡åˆ°é”™è¯¯æ—¶è·³è¿‡ç»§ç»­æ‰§è¡Œ')
        
        args = parser.parse_args()
        
        # æ ¹æ®æ¨¡å¼æ‰§è¡Œ
        if args.mode == 'complete':
            success = pipeline.run_complete_pipeline(skip_on_error=args.skip_on_error)
        elif args.mode == 'step':
            success = pipeline.run_step_by_step()
        elif args.mode == 'data':
            pipeline._initialize_modules()
            success = pipeline.run_data_processing()
        elif args.mode == 'sentiment':
            pipeline._initialize_modules()
            success = pipeline.run_sentiment_analysis()
        elif args.mode == 'frequency':
            pipeline._initialize_modules()
            success = pipeline.run_frequency_analysis()
        else:
            logger.error(f"æœªçŸ¥æ¨¡å¼: {args.mode}")
            success = False
        
        if success:
            logger.info("ğŸ‰ æ‰§è¡ŒæˆåŠŸ!")
            sys.exit(0)
        else:
            logger.error("ğŸ’¥ æ‰§è¡Œå¤±è´¥!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()