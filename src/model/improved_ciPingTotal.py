#!/usr/bin/env python3
"""
ä¿®å¤åçš„è¯é¢‘ç»Ÿè®¡æ¨¡å—
è§£å†³åŸæœ‰é€»è¾‘é—®é¢˜ï¼Œæå‡æ€§èƒ½å’Œå®‰å…¨æ€§
"""

import csv
import logging
import os
import re
import sys
from collections import Counter
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WordFrequencyAnalyzer:
    """è¯é¢‘åˆ†æå™¨ - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self, model_dir: str = None):
        self.model_dir = Path(model_dir) if model_dir else Path(__file__).parent

        # æ–‡ä»¶è·¯å¾„
        self.input_file = self.model_dir / 'comment_1_fenci.txt'
        self.output_file = self.model_dir / 'comment_1_fenci_qutingyongci_cipin.csv'
        self.stop_words_file = self.model_dir / 'stopWords.txt'

        # åŠ è½½åœç”¨è¯
        self.stop_words = self._load_stop_words()

        logger.info("è¯é¢‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_stop_words(self) -> set:
        """åŠ è½½åœç”¨è¯è¡¨"""
        stop_words = set()

        try:
            if self.stop_words_file.exists():
                with open(self.stop_words_file, 'r', encoding='utf-8') as f:
                    stop_words = {line.strip() for line in f if line.strip()}
                logger.info(f"åŠ è½½åœç”¨è¯ {len(stop_words)} ä¸ª")
            else:
                logger.warning(f"åœç”¨è¯æ–‡ä»¶ä¸å­˜åœ¨: {self.stop_words_file}")
                # ä½¿ç”¨é»˜è®¤åœç”¨è¯
                stop_words = {
                    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
                    'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
                    'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'èƒ½', 'è€Œ'
                }
        except Exception as e:
            logger.error(f"åŠ è½½åœç”¨è¯å¤±è´¥: {e}")
            stop_words = set()

        return stop_words

    def read_segmented_text(self) -> Optional[str]:
        """è¯»å–åˆ†è¯æ–‡æœ¬"""
        try:
            if not self.input_file.exists():
                logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {self.input_file}")
                return None

            with open(self.input_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()

            if not content:
                logger.warning("è¾“å…¥æ–‡ä»¶ä¸ºç©º")
                return None

            logger.info(f"è¯»å–åˆ†è¯æ–‡æœ¬ï¼Œé•¿åº¦: {len(content)}")
            return content

        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def filter_words(self, words: List[str]) -> List[str]:
        """è¿‡æ»¤æ— æ•ˆè¯è¯­"""
        filtered_words = []

        for word in words:
            word = word.strip()

            # è¿‡æ»¤æ¡ä»¶
            if (word and
                len(word) > 1 and                           # é•¿åº¦å¤§äº1
                word not in self.stop_words and             # ä¸åœ¨åœç”¨è¯è¡¨
                not re.match(r'^\d+$', word) and            # ä¸æ˜¯çº¯æ•°å­—
                not re.match(r'^[^\u4e00-\u9fa5a-zA-Z]+$', word) and  # ä¸æ˜¯çº¯ç¬¦å·
                len(word) <= 10):                           # é•¿åº¦ä¸è¶…è¿‡10ï¼ˆè¿‡æ»¤å¼‚å¸¸é•¿è¯ï¼‰
                filtered_words.append(word)

        return filtered_words

    def calculate_frequency(self, content: str, max_results: int = 300) -> List[Tuple[str, int]]:
        """è®¡ç®—è¯é¢‘ - æ”¹è¿›ç‰ˆ"""
        try:
            # åˆ†å‰²è¯è¯­
            words = content.split()

            if not words:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°è¯è¯­")
                return []

            # è¿‡æ»¤è¯è¯­
            filtered_words = self.filter_words(words)

            if not filtered_words:
                logger.warning("è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆè¯è¯­")
                return []

            # ä½¿ç”¨Counterç»Ÿè®¡è¯é¢‘ï¼ˆæ›´é«˜æ•ˆï¼‰
            word_counter = Counter(filtered_words)

            # è·å–æœ€å¸¸è§çš„è¯è¯­
            most_common = word_counter.most_common(max_results)

            logger.info(f"è¯é¢‘ç»Ÿè®¡å®Œæˆï¼Œå…± {len(most_common)} ä¸ªè¯è¯­")
            return most_common

        except Exception as e:
            logger.error(f"è¯é¢‘è®¡ç®—å¤±è´¥: {e}")
            return []

    def save_frequency_results(self, word_freq: List[Tuple[str, int]]) -> bool:
        """ä¿å­˜è¯é¢‘ç»“æœ - å®‰å…¨ç‰ˆ"""
        if not word_freq:
            logger.warning("è¯é¢‘ç»“æœä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
            return False

        try:
            # å¤‡ä»½åŸæ–‡ä»¶
            if self.output_file.exists():
                backup_file = self.output_file.with_suffix('.csv.bak')
                self.output_file.rename(backup_file)
                logger.info(f"åŸæ–‡ä»¶å·²å¤‡ä»½ä¸º: {backup_file}")

            # ä¿å­˜æ–°ç»“æœ
            with open(self.output_file, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                # å†™å…¥è¡¨å¤´
                writer.writerow(['è¯è¯­', 'é¢‘æ¬¡'])

                # å†™å…¥æ•°æ®
                for word, freq in word_freq:
                    writer.writerow([word, freq])

            logger.info(f"è¯é¢‘ç»“æœå·²ä¿å­˜: {self.output_file}")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜è¯é¢‘ç»“æœå¤±è´¥: {e}")
            return False

    def generate_frequency_report(self, word_freq: List[Tuple[str, int]]) -> Dict:
        """ç”Ÿæˆè¯é¢‘åˆ†ææŠ¥å‘Š"""
        if not word_freq:
            return {}

        try:
            total_words = sum(freq for _, freq in word_freq)
            unique_words = len(word_freq)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            frequencies = [freq for _, freq in word_freq]
            max_freq = max(frequencies)
            min_freq = min(frequencies)
            avg_freq = total_words / unique_words if unique_words > 0 else 0

            # é¢‘æ¬¡åˆ†å¸ƒ
            freq_distribution = {
                'é«˜é¢‘è¯(>10æ¬¡)': len([f for f in frequencies if f > 10]),
                'ä¸­é¢‘è¯(3-10æ¬¡)': len([f for f in frequencies if 3 <= f <= 10]),
                'ä½é¢‘è¯(1-2æ¬¡)': len([f for f in frequencies if f <= 2])
            }

            report = {
                'total_word_count': total_words,
                'unique_word_count': unique_words,
                'max_frequency': max_freq,
                'min_frequency': min_freq,
                'average_frequency': round(avg_freq, 2),
                'frequency_distribution': freq_distribution,
                'top_10_words': word_freq[:10]
            }

            logger.info(f"è¯é¢‘æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report}")
            return report

        except Exception as e:
            logger.error(f"ç”Ÿæˆè¯é¢‘æŠ¥å‘Šå¤±è´¥: {e}")
            return {}

    def run_frequency_analysis(self, max_results: int = 300) -> bool:
        """è¿è¡Œå®Œæ•´çš„è¯é¢‘åˆ†ææµç¨‹"""
        logger.info("å¼€å§‹æ‰§è¡Œè¯é¢‘åˆ†æ...")

        try:
            # 1. è¯»å–åˆ†è¯æ–‡æœ¬
            content = self.read_segmented_text()
            if not content:
                logger.error("æ— æ³•è¯»å–åˆ†è¯æ–‡æœ¬ï¼Œåˆ†æç»ˆæ­¢")
                return False

            # 2. è®¡ç®—è¯é¢‘
            word_freq = self.calculate_frequency(content, max_results)
            if not word_freq:
                logger.error("è¯é¢‘è®¡ç®—å¤±è´¥ï¼Œåˆ†æç»ˆæ­¢")
                return False

            # 3. ä¿å­˜ç»“æœ
            if not self.save_frequency_results(word_freq):
                logger.error("ä¿å­˜è¯é¢‘ç»“æœå¤±è´¥")
                return False

            # 4. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_frequency_report(word_freq)
            if report:
                # ä¿å­˜æŠ¥å‘Š
                report_file = self.model_dir / 'frequency_report.json'
                import json
                with open(report_file, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2)
                logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

            logger.info("âœ… è¯é¢‘åˆ†æå®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"è¯é¢‘åˆ†æå¼‚å¸¸: {e}")
            return False

    def get_top_words(self, n: int = 20) -> List[Tuple[str, int]]:
        """è·å–é«˜é¢‘è¯TOP N"""
        try:
            if not self.output_file.exists():
                logger.error(f"è¯é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {self.output_file}")
                return []

            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(self.output_file)

            # è·å–å‰Nä¸ªè¯è¯­
            top_words = []
            for _, row in df.head(n).iterrows():
                top_words.append((row['è¯è¯­'], int(row['é¢‘æ¬¡'])))

            logger.info(f"è·å–TOP {n} é«˜é¢‘è¯å®Œæˆ")
            return top_words

        except Exception as e:
            logger.error(f"è·å–é«˜é¢‘è¯å¤±è´¥: {e}")
            return []

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºè¯é¢‘åˆ†æå™¨
        analyzer = WordFrequencyAnalyzer()

        # è¿è¡Œåˆ†æ
        success = analyzer.run_frequency_analysis(max_results=300)

        if success:
            logger.info("ğŸ‰ è¯é¢‘åˆ†ææˆåŠŸ!")

            # æ˜¾ç¤ºTOP 10
            top_words = analyzer.get_top_words(10)
            if top_words:
                logger.info("ğŸ“Š TOP 10 é«˜é¢‘è¯:")
                for i, (word, freq) in enumerate(top_words, 1):
                    logger.info(f"  {i:2d}. {word} ({freq}æ¬¡)")
        else:
            logger.error("ğŸ’¥ è¯é¢‘åˆ†æå¤±è´¥!")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()