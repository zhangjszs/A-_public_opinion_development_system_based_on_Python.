#!/usr/bin/env python3
"""
ä¿®å¤åçš„èˆ†æƒ…åˆ†æä¸»æ§åˆ¶å™¨
è§£å†³åŸæœ‰é€»è¾‘é—®é¢˜ï¼Œæå‡å®‰å…¨æ€§å’Œç¨³å®šæ€§
"""

import logging
import os
import pickle
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SentimentAnalyzer:
    """æƒ…æ„Ÿåˆ†ææ§åˆ¶å™¨ - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self, model_dir: str = None):
        self.model_dir = Path(model_dir) if model_dir else Path(__file__).parent
        self.model_path = self.model_dir / 'best_sentiment_model.pkl'
        self.model = None

        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.target_csv = self.model_dir / 'target.csv'
        self.freq_csv = self.model_dir / 'comment_1_fenci_qutingyongci_cipin.csv'

        logger.info("æƒ…æ„Ÿåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def load_model(self) -> bool:
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ - å®‰å…¨ç‰ˆ"""
        try:
            if not self.model_path.exists():
                logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
                return False

            with open(self.model_path, 'rb') as f:
                self.model = pickle.load(f)

            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            return True

        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def get_comment_data(self) -> List[List]:
        """è·å–è¯„è®ºæ•°æ® - æ”¹è¿›ç‰ˆ"""
        try:
            # å°è¯•å¯¼å…¥æ•°æ®è·å–æ¨¡å—
            sys.path.append(str(self.model_dir.parent / 'utils'))
            from getPublicData import getAllCommentsData

            comment_list = getAllCommentsData()
            if not comment_list:
                logger.warning("æœªè·å–åˆ°è¯„è®ºæ•°æ®")
                return []

            logger.info(f"è·å–åˆ° {len(comment_list)} æ¡è¯„è®ºæ•°æ®")
            return comment_list

        except ImportError as e:
            logger.error(f"å¯¼å…¥æ•°æ®æ¨¡å—å¤±è´¥: {e}")
            # å°è¯•è¯»å–CSVæ–‡ä»¶ä½œä¸ºå¤‡ç”¨
            return self._load_from_csv_backup()
        except Exception as e:
            logger.error(f"è·å–è¯„è®ºæ•°æ®å¤±è´¥: {e}")
            return []

    def _load_from_csv_backup(self) -> List[List]:
        """ä»CSVæ–‡ä»¶è¯»å–æ•°æ®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        try:
            csv_files = [
                self.model_dir.parent / 'spider' / 'commentsData.csv',
                self.model_dir.parent / 'cache' / 'comments.csv'
            ]

            for csv_file in csv_files:
                if csv_file.exists():
                    df = pd.read_csv(csv_file)
                    logger.info(f"ä»å¤‡ç”¨CSVæ–‡ä»¶è¯»å–æ•°æ®: {csv_file}")
                    return df.values.tolist()

            logger.warning("æœªæ‰¾åˆ°å¤‡ç”¨CSVæ–‡ä»¶")
            return []

        except Exception as e:
            logger.error(f"è¯»å–å¤‡ç”¨CSVæ–‡ä»¶å¤±è´¥: {e}")
            return []

    def preprocess_comments(self, comment_list: List[List]) -> List[str]:
        """é¢„å¤„ç†è¯„è®ºæ–‡æœ¬ - æ”¹è¿›ç‰ˆ"""
        if not comment_list:
            logger.warning("è¯„è®ºåˆ—è¡¨ä¸ºç©º")
            return []

        try:
            processed_texts = []

            for comment in comment_list:
                if len(comment) > 4 and comment[4]:
                    text = str(comment[4]).strip()

                    # æ–‡æœ¬æ¸…æ´—
                    if text and len(text) > 2:  # è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬
                        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡
                        import re
                        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
                        text = re.sub(r'\s+', ' ', text).strip()

                        if text:
                            processed_texts.append(text)

            logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ–‡æœ¬ {len(processed_texts)} æ¡")
            return processed_texts

        except Exception as e:
            logger.error(f"æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: {e}")
            return []

    def analyze_sentiment(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æƒ…æ„Ÿåˆ†æ - æ”¹è¿›ç‰ˆ"""
        if not texts:
            logger.warning("æ–‡æœ¬åˆ—è¡¨ä¸ºç©º")
            return []

        if not self.model:
            logger.error("æ¨¡å‹æœªåŠ è½½")
            return []

        try:
            results = []

            for i, text in enumerate(texts):
                try:
                    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹
                    prediction = self.model.predict([text])
                    probability = self.model.predict_proba([text])

                    # æƒ…æ„Ÿåˆ†ç±»
                    sentiment_map = {0: 'è´Ÿé¢', 1: 'ä¸­æ€§', 2: 'æ­£é¢'}
                    sentiment = sentiment_map.get(prediction[0], 'æœªçŸ¥')

                    # ç½®ä¿¡åº¦
                    confidence = float(np.max(probability))

                    results.append({
                        'text': text,
                        'sentiment': sentiment,
                        'confidence': confidence,
                        'raw_prediction': int(prediction[0])
                    })

                except Exception as e:
                    logger.warning(f"åˆ†æç¬¬{i}æ¡æ–‡æœ¬å¤±è´¥: {e}")
                    results.append({
                        'text': text,
                        'sentiment': 'æœªçŸ¥',
                        'confidence': 0.0,
                        'raw_prediction': -1
                    })

            logger.info(f"æƒ…æ„Ÿåˆ†æå®Œæˆï¼Œå¤„ç† {len(results)} æ¡æ–‡æœ¬")
            return results

        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return []

    def save_analysis_results(self, results: List[Dict[str, Any]]) -> bool:
        """ä¿å­˜åˆ†æç»“æœ - å®‰å…¨ç‰ˆ"""
        if not results:
            logger.warning("ç»“æœåˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
            return False

        try:
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(results)

            # æ·»åŠ æ—¶é—´æˆ³
            df['analysis_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # ä¿å­˜ä¸ºCSV
            output_file = self.model_dir / f'sentiment_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
            df.to_csv(output_file, index=False, encoding='utf-8')

            # åŒæ—¶æ›´æ–°ç›®æ ‡æ–‡ä»¶
            if self.target_csv.exists():
                # å¤‡ä»½åŸæ–‡ä»¶
                backup_file = self.target_csv.with_suffix('.csv.bak')
                if self.target_csv.exists():
                    self.target_csv.rename(backup_file)

            df.to_csv(self.target_csv, index=False, encoding='utf-8')

            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜: {output_file}")
            logger.info(f"ç›®æ ‡æ–‡ä»¶å·²æ›´æ–°: {self.target_csv}")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return False

    def generate_summary_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡ - æ–°å¢åŠŸèƒ½"""
        if not results:
            return {}

        try:
            total_count = len(results)
            sentiment_counts = {}
            confidence_sum = 0

            for result in results:
                sentiment = result.get('sentiment', 'æœªçŸ¥')
                confidence = result.get('confidence', 0)

                sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
                confidence_sum += confidence

            # è®¡ç®—æ¯”ä¾‹
            sentiment_ratios = {
                sentiment: count / total_count
                for sentiment, count in sentiment_counts.items()
            }

            summary = {
                'total_comments': total_count,
                'sentiment_counts': sentiment_counts,
                'sentiment_ratios': sentiment_ratios,
                'average_confidence': confidence_sum / total_count if total_count > 0 else 0,
                'analysis_time': datetime.now().isoformat()
            }

            logger.info(f"ç»Ÿè®¡æ±‡æ€»å®Œæˆ: {summary}")
            return summary

        except Exception as e:
            logger.error(f"ç”Ÿæˆç»Ÿè®¡æ±‡æ€»å¤±è´¥: {e}")
            return {}

    def run_analysis_pipeline(self) -> bool:
        """è¿è¡Œå®Œæ•´åˆ†ææµæ°´çº¿ - ä¸»è¦æ¥å£"""
        logger.info("å¼€å§‹æ‰§è¡Œèˆ†æƒ…åˆ†ææµæ°´çº¿...")

        try:
            # 1. åŠ è½½æ¨¡å‹
            if not self.load_model():
                logger.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return False

            # 2. è·å–è¯„è®ºæ•°æ®
            comment_list = self.get_comment_data()
            if not comment_list:
                logger.error("æ— æ³•è·å–è¯„è®ºæ•°æ®ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return False

            # 3. é¢„å¤„ç†æ–‡æœ¬
            processed_texts = self.preprocess_comments(comment_list)
            if not processed_texts:
                logger.error("æ–‡æœ¬é¢„å¤„ç†å¤±è´¥ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return False

            # 4. æƒ…æ„Ÿåˆ†æ
            analysis_results = self.analyze_sentiment(processed_texts)
            if not analysis_results:
                logger.error("æƒ…æ„Ÿåˆ†æå¤±è´¥ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return False

            # 5. ä¿å­˜ç»“æœ
            if not self.save_analysis_results(analysis_results):
                logger.error("ä¿å­˜ç»“æœå¤±è´¥ï¼Œä½†åˆ†æå·²å®Œæˆ")

            # 6. ç”Ÿæˆç»Ÿè®¡æ±‡æ€»
            summary = self.generate_summary_statistics(analysis_results)
            if summary:
                # ä¿å­˜ç»Ÿè®¡æ±‡æ€»
                summary_file = self.model_dir / 'analysis_summary.json'
                import json
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump(summary, f, ensure_ascii=False, indent=2)
                logger.info(f"ç»Ÿè®¡æ±‡æ€»å·²ä¿å­˜: {summary_file}")

            logger.info("âœ… èˆ†æƒ…åˆ†ææµæ°´çº¿æ‰§è¡ŒæˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"åˆ†ææµæ°´çº¿å¼‚å¸¸: {e}")
            return False

    def safe_cleanup(self, keep_results: bool = True):
        """å®‰å…¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        temp_files = []

        # å®šä¹‰ä¸´æ—¶æ–‡ä»¶ï¼ˆä¿ç•™é‡è¦ç»“æœæ–‡ä»¶ï¼‰
        if not keep_results:
            temp_files.extend([
                self.model_dir / 'comment_1_fenci.txt',
                self.model_dir / 'temp_analysis.csv'
            ])

        for file_path in temp_files:
            try:
                if file_path.exists():
                    file_path.unlink()
                    logger.info(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

def main():
    """ä¸»å‡½æ•° - ç¨‹åºå…¥å£"""
    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = SentimentAnalyzer()

        # æ‰§è¡Œåˆ†ææµæ°´çº¿
        success = analyzer.run_analysis_pipeline()

        if success:
            logger.info("ğŸ‰ èˆ†æƒ…åˆ†æå®Œæˆ!")
        else:
            logger.error("ğŸ’¥ èˆ†æƒ…åˆ†æå¤±è´¥!")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()